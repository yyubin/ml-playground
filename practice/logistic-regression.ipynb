{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM+ZLItxjvVvLW/T6/px38i"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 1. 퍼셉트론(Perceptron)\n","\n","- 최초의 인공 뉴런 모델\n","- 이진 분류기 (출력: 0 or 1)\n","\n","### 동작 방식\n","\n","$$\n","\\hat{y}=\n","\\begin{cases}\n","1 & w\\cdot x+b > 0\\\\\n","0 & otherwise\n","\\end{cases}\n","$$\n","\n","- 입력 $x$에 대해 가중치 $w$와 절편 $b$를 적용하고\n","- 계단 함수(step function)로 결과를 0 또는 1로 출력\n","\n","### 학습 방법\n","\n","- 예측이 틀릴 때만 업데이트\n","\n","$$\n","w := w+\\alpha(y-\\hat{y})x\n","$$\n","\n","### 문제점\n","\n","- 선형적으로 분리되지 않는 데이터(XOR 문제 등)는 학습 불가\n","- 계단 함수는 미분이 불가능해서 수학적으로 최적화가 어려움\n","\n","# 2. 아달린(Adaline: Adaptive Linear Neuron)\n","\n","- 퍼셉트론의 개선형\n","- 출력은 0/1이지만, 학습은 **출력 전의 연속값**을 기준으로 함\n","\n","### 동작 방식\n","\n","$$\n","Net\\ input\\ z = w\\cdot x+b\n","$$\n","\n","$$\n","\\hat{y}=\n","\\begin{cases}\n","1 & z > 0\\\\\n","0 & otherwise\n","\\end{cases}\n","$$\n","\n","하지만 학습에서는 **z값 자체를 사용**\n","\n","$$\n","L(w)=\\sum(y-z)^2=\\sum(y-(w\\cdot x+b))^2\n","$$\n","\n","- **제곱 오차 손실 함수(MSE)**사용\n","- 경사하강법 가능(z는 연속이므로 미분 가능)\n","\n","### 문제점\n","\n","- 출력은 여전히 선형 모델, 분류에 부적합할 가능성있음\n","- 예측을 0/1로 자르기 때문에 확률적 의미 부족\n","\n","# 3. 로지스틱 회귀(Logistic Regression)\n","\n","- 아달린의 문제점을 극복한 모델\n","- 분류 문제에 확률적 해석 제공\n","\n","### 동작 방식\n","\n","$$\n","\\hat{y}=\\sigma(z)=\\frac{1}{1+e^{-z}}, \\ where \\ z = w\\cdot x+b\n","$$\n","\n","- **시그모이드 함수를 사용해 출력을 (0, 1) 사이의 확률로 변환**\n","- 출력이 0.5보다 크면 1로 작으면 0으로 판단\n","\n","### 손실 함수\n","\n","$$\n","L(w) = -\\sum[y\\log(\\hat{y})+(1-y)\\log(1-\\hat y)]\n","$$\n","\n","- 경사하강법 적용 가능\n","- 분류 확률을 직접 모델링하기 때문에 이론적 해석이 뛰어남\n","\n","| 항목 | 퍼셉트론 | 아달린 (Adaline) | 로지스틱 회귀 |\n","| --- | --- | --- | --- |\n","| 출력 방식 | 이진 출력 (0 or 1) | 이진 출력, 학습은 연속값 | 확률 (0~1) |\n","| 활성화 함수 | 계단 함수 | 항등 함수 | 시그모이드 함수 |\n","| 손실 함수 | 없음 (단순 조건 기반) | MSE (제곱 오차) | Cross Entropy (로그 손실) |\n","| 미분 가능성 | ❌ (계단 함수) | ✅ | ✅ |\n","| 학습 방식 | 틀린 경우만 업데이트 | 경사하강법 사용 | 경사하강법 사용 |\n","| 확률 해석 | ❌ | ❌ | ✅ |\n","| 주요 한계 | 선형 분리만 가능, 미분 불가 | 분류 문제에 약함 | 비교적 이상적 |"],"metadata":{"id":"YYrp6V5jERvO"}},{"cell_type":"markdown","source":["# 오즈 비, 로짓 함수, 로지스틱 함수(=시그모이드)"],"metadata":{"id":"1nLsLyIRNiBr"}},{"cell_type":"markdown","source":["# 1. 오즈와 오즈비(Odds & Odds Ratio)\n","\n","### 오즈\n","\n","- 어떤 사건이 일어날 확률을 $p$, 일어나지 않을 확률을 $1-p$라고 하면\n","\n","$$\n","Odds=\\frac{p}{1-p}\n","$$\n","\n","성공확률과 실패확률의 비율을 의미함\n","\n","- $p=0.8$\n","- $Odds=\\frac{0.8}{0.2}=4$\n","    - 성공이 실패보다 4배더 일어날 확률이 있음\n","\n","### 오즈 비\n","\n","- 두 집단 간의 오즈 비율 비교\n","\n","$$\n","Odds\\ Ratio=\\frac{p_1/(1-p_1)}{p_2/(1-p_2)}\n","$$\n","\n","특정 약 복용 그룹 vs 비복용 그룹의 질병 발생 오즈를 비교 하는 식으로 사용\n","\n","# 2. 로짓 함수(Logit Fucntion)\n","\n","로짓 함수는 **오즈의 로그값**\n","\n","$$\n","logit(p)=\\log(\\frac{p}{1-p})\n","$$\n","\n","이 함수는 **(0, 1)사이의 확률 $p$를 실수 전체 범위 $(-\\infty, \\infty)$로 바꿔줌**\n","\n","### 사용 이유\n","\n","- 로지스틱 회귀는 입력 $x$에 대해 선형결합 $z=w\\cdot x+b$를 사용함\n","- 그런데 선형결합의 결과 $z\\in \\mathbb{R}$를 확률로 해석하려면 $z$를 (0, 1)로 바꿔야 한다\n","- 그래서 확률 $p$를 $z$로 바꿔주는 함수가 logit임\n","\n","$$\n","z=\\log(\\frac{p}{1-p}) \\iff p=\\frac{1}{1+e^{-z}}\n","$$\n","\n","# 3. 로지스틱 함수 = 시그모이드 함수\n","\n","$$\n","\\sigma(z)=\\frac{1}{1+e^{-z}}\n","$$\n","\n","- 입력 $z \\in (-\\infty, \\infty)$에 대해, 출력은 항상 $\\in (0, 1)$\n","- 확률처럼 해석 가능\n","\n","### 로지스틱 회귀와의 관계\n","\n","- 로지스틱 회귀는 아래와 같음\n","\n","$$\n","p(y=1|x)=\\sigma(w\\cdot x+b)\n","$$\n","\n","- 선형 회귀처럼 $wx+b$를 계산하고 **시그모이드로 확률을 변환**\n","\n","# 결론\n","\n","1. **오즈**는 $\\frac{p}{1-p}$형태로 확률을 비교하는 방식\n","2. **로짓**은 오즈에 로그를 씌운 것 → 선형 모델과 연결 가능\n","3. **시그모이드**는 로짓 함수의 역함수 → 확률 출력 가능\n","\n","$$\n","확률p\\xrightarrow{odds} \\frac{p}{1-p} \\xrightarrow{log} \\log(\\frac{p}{1-p})=z\\xrightarrow{sigmoid^{-1}} \\frac{1}{1+e^{-z}}\n","$$"],"metadata":{"id":"Ibp5Rh3zNlLZ"}},{"cell_type":"markdown","source":["### 1. 로지스틱 손실함수 (Binary Cross-Entropy)\n","\n","선형 결합\n","\n","$$\n","z = w^{\\top}x + b\n","$$\n","\n","시그모이드(로지스틱) 활성화\n","\n","$$\n","a = \\sigma(z)=\\frac{1}{1+e^{-z}}\\qquad(0<a<1)\n","$$\n","\n","이때 **로그우도 최대화** ↔ **Binary Cross-Entropy(BCE) 최소화**\n","\n","$$\n","\\boxed{ \\;L(a,y)= -\\bigl[y\\;\\log a + (1-y)\\,\\log(1-a)\\bigr] \\;}\n","$$\n","\n","(표본 1 개 기준, 미니배치라면 평균을 취하면 됨)\n","\n","---\n","\n","### 2. 미분 준비: 시그모이드의 도함수\n","\n","$$\n","\\sigma'(z)=\\frac{d}{dz}\\frac{1}{1+e^{-z}}\n","           =\\sigma(z)\\bigl(1-\\sigma(z)\\bigr)\n","           =a(1-a)\n","$$\n","\n","---\n","\n","### 3. 손실을 **z**까지 미분하기\n","\n","(역전파의 핵심 *local gradient*)\n","\n","$$\n","\\frac{\\partial L}{\\partial a}\n","      =-\\Bigl[\\frac{y}{a}-\\frac{1-y}{1-a}\\Bigr]\n","      = -\\frac{y(1-a)-(1-y)a}{a(1-a)}\n","      = \\frac{a-y}{a(1-a)}\n","$$\n","\n","$$\n","\\frac{\\partial L}{\\partial z}\n","      =\\frac{\\partial L}{\\partial a}\\;\\frac{\\partial a}{\\partial z}\n","      =\\Bigl[\\frac{a-y}{a(1-a)}\\Bigr]\\;a(1-a)\n","      = a-y\n","$$\n","\n","> **중요 결과**\n",">\n","> $$\n","> \\boxed{\\displaystyle \\frac{\\partial L}{\\partial z}=a-y}\n","> $$\n",">\n","> MSE에서 $(\\hat{y}-y)$가 나오듯 BCE에서도 **“예측-실제”** 형태가 유지된다\n","> 다만 $\\hat{y}$ 대신 **활성화값 $a$** 가 등장한다\n","\n","---\n","\n","### 4. 가중치 & 절편 기울기\n","\n","$$\n","\\frac{\\partial L}{\\partial w}\n","   =\\frac{\\partial L}{\\partial z}\\;\\frac{\\partial z}{\\partial w}\n","   =(a-y)\\;x\n","$$\n","\n","$$\n","\\frac{\\partial L}{\\partial b}\n","   =\\frac{\\partial L}{\\partial z}\\;\\frac{\\partial z}{\\partial b}\n","   =(a-y)\\;(1)=a-y\n","$$\n","\n","벡터/미니배치라면 표본 $i$ 들에 대해 합 또는 평균을 취하면 됨\n","\n","$$\n","\\boxed{\\displaystyle\n","\\nabla_w L = \\frac{1}{m}\\sum_{i=1}^{m}(a^{(i)}-y^{(i)})\\,x^{(i)},\\qquad\n","\\frac{\\partial L}{\\partial b} = \\frac{1}{m}\\sum_{i=1}^{m}(a^{(i)}-y^{(i)})\n","}\n","$$\n","\n","---\n","\n","### 5. 경사하강법 업데이트\n","\n","학습률 $\\alpha$:\n","\n","$$\n","w \\;\\leftarrow\\; w - \\alpha \\,\\nabla_w L\n","          = w - \\alpha \\,\\frac{1}{m}\\sum_{i}(a^{(i)}-y^{(i)})x^{(i)}\n","$$\n","\n","$$\n","b \\;\\leftarrow\\; b - \\alpha \\,\\frac{\\partial L}{\\partial b}\n","          = b - \\alpha \\,\\frac{1}{m}\\sum_{i}(a^{(i)}-y^{(i)})\n","$$\n","\n","---\n","\n","### 6. 역전파 관점 정리\n","\n","1. **순전파**: $z\\rightarrow a\\rightarrow L$ 계산\n","2. **역전파**:\n","\n","   * $\\delta = a-y$ (출력층 오차)\n","   * 가중치 기울기: $\\delta x$\n","   * 편향 기울기: $\\delta$\n","\n","   다층 신경망에서는 이 $\\delta$가 다음 층으로 전파되며\n","\n","   $$\n","   \\delta^{(l)} = \\bigl((W^{(l+1)})^\\top\\delta^{(l+1)}\\bigr)\\odot g'^{(l)}(z^{(l)})\n","   $$\n","\n","---\n","\n","### 7. 핵심만 기억하기\n","\n","| 단계          | 수식                                                           | 의미        |\n","| ----------- | ------------------------------------------------------------ | --------- |\n","| **손실**      | $L=-[y\\log a+(1-y)\\log(1-a)]$                                | BCE       |\n","| **출력층 오차**  | $\\delta=a-y$                                                 | “예측–실제”   |\n","| **가중치 기울기** | $\\partial L/\\partial w = \\delta x$                           | 입력과 오차의 곱 |\n","| **편향 기울기**  | $\\partial L/\\partial b = \\delta$                             | 오차 자체     |\n","| **업데이트**    | $w\\leftarrow w-\\alpha\\delta x,\\; b\\leftarrow b-\\alpha\\delta$ | 경사하강      |\n","\n","이 과정을 그대로 코드에 옮기면 **로지스틱 회귀 학습**이 구현되고, 동일한 역전파 아이디어가 다층 신경망까지 확장가능\n"],"metadata":{"id":"SACjaVEGWWK9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pQdPSjblEC42"},"outputs":[],"source":[]}]}