{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO3iswXTdCETE7eJf0aKPtc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 1. 퍼셉트론(Perceptron)\n","\n","- 최초의 인공 뉴런 모델\n","- 이진 분류기 (출력: 0 or 1)\n","\n","### 동작 방식\n","\n","$$\n","\\hat{y}=\n","\\begin{cases}\n","1 & w\\cdot x+b > 0\\\\\n","0 & otherwise\n","\\end{cases}\n","$$\n","\n","- 입력 $x$에 대해 가중치 $w$와 절편 $b$를 적용하고\n","- 계단 함수(step function)로 결과를 0 또는 1로 출력\n","\n","### 학습 방법\n","\n","- 예측이 틀릴 때만 업데이트\n","\n","$$\n","w := w+\\alpha(y-\\hat{y})x\n","$$\n","\n","### 문제점\n","\n","- 선형적으로 분리되지 않는 데이터(XOR 문제 등)는 학습 불가\n","- 계단 함수는 미분이 불가능해서 수학적으로 최적화가 어려움\n","\n","# 2. 아달린(Adaline: Adaptive Linear Neuron)\n","\n","- 퍼셉트론의 개선형\n","- 출력은 0/1이지만, 학습은 **출력 전의 연속값**을 기준으로 함\n","\n","### 동작 방식\n","\n","$$\n","Net\\ input\\ z = w\\cdot x+b\n","$$\n","\n","$$\n","\\hat{y}=\n","\\begin{cases}\n","1 & z > 0\\\\\n","0 & otherwise\n","\\end{cases}\n","$$\n","\n","하지만 학습에서는 **z값 자체를 사용**\n","\n","$$\n","L(w)=\\sum(y-z)^2=\\sum(y-(w\\cdot x+b))^2\n","$$\n","\n","- **제곱 오차 손실 함수(MSE)**사용\n","- 경사하강법 가능(z는 연속이므로 미분 가능)\n","\n","### 문제점\n","\n","- 출력은 여전히 선형 모델, 분류에 부적합할 가능성있음\n","- 예측을 0/1로 자르기 때문에 확률적 의미 부족\n","\n","# 3. 로지스틱 회귀(Logistic Regression)\n","\n","- 아달린의 문제점을 극복한 모델\n","- 분류 문제에 확률적 해석 제공\n","\n","### 동작 방식\n","\n","$$\n","\\hat{y}=\\sigma(z)=\\frac{1}{1+e^{-z}}, \\ where \\ z = w\\cdot x+b\n","$$\n","\n","- **시그모이드 함수를 사용해 출력을 (0, 1) 사이의 확률로 변환**\n","- 출력이 0.5보다 크면 1로 작으면 0으로 판단\n","\n","### 손실 함수\n","\n","$$\n","L(w) = -\\sum[y\\log(\\hat{y})+(1-y)\\log(1-\\hat y)]\n","$$\n","\n","- 경사하강법 적용 가능\n","- 분류 확률을 직접 모델링하기 때문에 이론적 해석이 뛰어남\n","\n","| 항목 | 퍼셉트론 | 아달린 (Adaline) | 로지스틱 회귀 |\n","| --- | --- | --- | --- |\n","| 출력 방식 | 이진 출력 (0 or 1) | 이진 출력, 학습은 연속값 | 확률 (0~1) |\n","| 활성화 함수 | 계단 함수 | 항등 함수 | 시그모이드 함수 |\n","| 손실 함수 | 없음 (단순 조건 기반) | MSE (제곱 오차) | Cross Entropy (로그 손실) |\n","| 미분 가능성 | ❌ (계단 함수) | ✅ | ✅ |\n","| 학습 방식 | 틀린 경우만 업데이트 | 경사하강법 사용 | 경사하강법 사용 |\n","| 확률 해석 | ❌ | ❌ | ✅ |\n","| 주요 한계 | 선형 분리만 가능, 미분 불가 | 분류 문제에 약함 | 비교적 이상적 |"],"metadata":{"id":"YYrp6V5jERvO"}},{"cell_type":"markdown","source":["# 오즈 비, 로짓 함수, 로지스틱 함수(=시그모이드)"],"metadata":{"id":"1nLsLyIRNiBr"}},{"cell_type":"markdown","source":["# 1. 오즈와 오즈비(Odds & Odds Ratio)\n","\n","### 오즈\n","\n","- 어떤 사건이 일어날 확률을 $p$, 일어나지 않을 확률을 $1-p$라고 하면\n","\n","$$\n","Odds=\\frac{p}{1-p}\n","$$\n","\n","성공확률과 실패확률의 비율을 의미함\n","\n","- $p=0.8$\n","- $Odds=\\frac{0.8}{0.2}=4$\n","    - 성공이 실패보다 4배더 일어날 확률이 있음\n","\n","### 오즈 비\n","\n","- 두 집단 간의 오즈 비율 비교\n","\n","$$\n","Odds\\ Ratio=\\frac{p_1/(1-p_1)}{p_2/(1-p_2)}\n","$$\n","\n","특정 약 복용 그룹 vs 비복용 그룹의 질병 발생 오즈를 비교 하는 식으로 사용\n","\n","# 2. 로짓 함수(Logit Fucntion)\n","\n","로짓 함수는 **오즈의 로그값**\n","\n","$$\n","logit(p)=\\log(\\frac{p}{1-p})\n","$$\n","\n","이 함수는 **(0, 1)사이의 확률 $p$를 실수 전체 범위 $(-\\infty, \\infty)$로 바꿔줌**\n","\n","### 사용 이유\n","\n","- 로지스틱 회귀는 입력 $x$에 대해 선형결합 $z=w\\cdot x+b$를 사용함\n","- 그런데 선형결합의 결과 $z\\in \\mathbb{R}$를 확률로 해석하려면 $z$를 (0, 1)로 바꿔야 한다\n","- 그래서 확률 $p$를 $z$로 바꿔주는 함수가 logit임\n","\n","$$\n","z=\\log(\\frac{p}{1-p}) \\iff p=\\frac{1}{1+e^{-z}}\n","$$\n","\n","# 3. 로지스틱 함수 = 시그모이드 함수\n","\n","$$\n","\\sigma(z)=\\frac{1}{1+e^{-z}}\n","$$\n","\n","- 입력 $z \\in (-\\infty, \\infty)$에 대해, 출력은 항상 $\\in (0, 1)$\n","- 확률처럼 해석 가능\n","\n","### 로지스틱 회귀와의 관계\n","\n","- 로지스틱 회귀는 아래와 같음\n","\n","$$\n","p(y=1|x)=\\sigma(w\\cdot x+b)\n","$$\n","\n","- 선형 회귀처럼 $wx+b$를 계산하고 **시그모이드로 확률을 변환**\n","\n","# 결론\n","\n","1. **오즈**는 $\\frac{p}{1-p}$형태로 확률을 비교하는 방식\n","2. **로짓**은 오즈에 로그를 씌운 것 → 선형 모델과 연결 가능\n","3. **시그모이드**는 로짓 함수의 역함수 → 확률 출력 가능\n","\n","$$\n","확률p\\xrightarrow{odds} \\frac{p}{1-p} \\xrightarrow{log} \\log(\\frac{p}{1-p})=z\\xrightarrow{sigmoid^{-1}} \\frac{1}{1+e^{-z}}\n","$$"],"metadata":{"id":"Ibp5Rh3zNlLZ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pQdPSjblEC42"},"outputs":[],"source":[]}]}